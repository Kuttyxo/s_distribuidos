# Dockerfile.hadoop-pig (Versión más moderna y autodependiente)
FROM ubuntu:20.04

LABEL maintainer="Tu Nombre <tu.email@example.com>"

# Configurar locale para evitar advertencias de Python
ENV LANG C.UTF-8
ENV LC_ALL C.UTF-8

# Instalar Java 11 (requerido por Hadoop y Pig) y otras utilidades
RUN apt-get update && \
    DEBIAN_FRONTEND=noninteractive apt-get install -y openjdk-11-jdk wget curl gnupg procps net-tools && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*

ENV JAVA_HOME /usr/lib/jvm/java-11-openjdk-amd64
ENV PATH $PATH:$JAVA_HOME/bin

# Instalar y configurar Hadoop 3.3.6
ENV HADOOP_VERSION 3.3.6
ENV HADOOP_HOME /usr/local/hadoop
ENV PATH $PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

RUN wget -q https://downloads.apache.org/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz -O /tmp/hadoop.tar.gz && \
    tar -xzf /tmp/hadoop.tar.gz -C /usr/local && \
    mv /usr/local/hadoop-${HADOOP_VERSION} ${HADOOP_HOME} && \
    rm /tmp/hadoop.tar.gz

# Configurar permisos para Hadoop
RUN chown -R root:root ${HADOOP_HOME}

# Configurar Hadoop para Single Node Cluster (pseudo-distribuido)
# Crea los directorios necesarios para HDFS
RUN mkdir -p ${HADOOP_HOME}/hdfs/namenode && \
    mkdir -p ${HADOOP_HOME}/hdfs/datanode

# Core-site.xml: Configura el sistema de archivos por defecto como HDFS
COPY hadoop_configs/core-site.xml ${HADOOP_HOME}/etc/hadoop/
# HDFS-site.xml: Configura los directorios de NameNode/DataNode
COPY hadoop_configs/hdfs-site.xml ${HADOOP_HOME}/etc/hadoop/
# Mapred-site.xml: Usa YARN como framework de MapReduce
COPY hadoop_configs/mapred-site.xml ${HADOOP_HOME}/etc/hadoop/
# Yarn-site.xml: Habilita el log aggregation de YARN
COPY hadoop_configs/yarn-site.xml ${HADOOP_HOME}/etc/hadoop/
# hadoop-env.sh: Configura JAVA_HOME para Hadoop
COPY hadoop_configs/hadoop-env.sh ${HADOOP_HOME}/etc/hadoop/

# Instalar Pig
ENV PIG_VERSION 0.17.0
ENV PIG_HOME /usr/local/pig
ENV PATH $PATH:$PIG_HOME/bin

RUN wget -q https://downloads.apache.org/pig/pig-${PIG_VERSION}/pig-${PIG_VERSION}.tar.gz -O /tmp/pig.tar.gz && \
    tar -xzf /tmp/pig.tar.gz -C /usr/local && \
    mv /usr/local/pig-${PIG_VERSION} ${PIG_HOME} && \
    rm /tmp/pig.tar.gz

# Descargar el conector Pig-MongoDB (AJUSTA LA VERSIÓN si encuentras una más reciente y compatible)
# Puedes buscar versiones más recientes aquí: https://mvnrepository.com/artifact/org.mongodb/mongo-hadoop-core
ENV MONGO_HADOOP_VERSION 2.0.2
RUN wget -q https://repo1.maven.org/maven2/org/mongodb/mongo-hadoop/mongo-hadoop-core/${MONGO_HADOOP_VERSION}/mongo-hadoop-core-${MONGO_HADOOP_VERSION}.jar -O ${PIG_HOME}/lib/mongo-hadoop-core-${MONGO_HADOOP_VERSION}.jar && \
    wget -q https://repo1.maven.org/maven2/org/mongodb/mongo-hadoop/mongo-hadoop-pig/${MONGO_HADOOP_VERSION}/mongo-hadoop-pig-${MONGO_HADOOP_VERSION}.jar -O ${PIG_HOME}/lib/mongo-hadoop-pig-${MONGO_HADOOP_VERSION}.jar

# Copiar el script de inicio para formatear HDFS y levantar servicios
COPY hadoop-startup.sh /usr/local/bin/hadoop-startup.sh
RUN chmod +x /usr/local/bin/hadoop-startup.sh

# Establecer directorio de trabajo
WORKDIR /app

# Exponer puertos si es necesario
EXPOSE 8088 9870 9000

# Comando de inicio del contenedor (format HDFS and start Hadoop services)
CMD ["/usr/local/bin/hadoop-startup.sh"]